{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fantastic-invitation",
   "metadata": {},
   "source": [
    "# Machine Learning with Python\n",
    "\n",
    "The aim of this lab is to implement a complete ML project in python. We will go through the steps of data exploration, cleaning and transformation followed by model training and selection.\n",
    "\n",
    "The lab consists of two parts. The first part is an interactive tutorial adapted from [Aurélien Géron's excellent ML book](https://github.com/ageron/handson-ml2) with some modifications to make it a classification tutorial instead of a regression one. We will learn to classify neighborhoods by median house value. The tutorial will contains conceptual questions as well as fill-in code that we'll give you a few minutes to write.\n",
    "\n",
    "In the second part of this lab, we expect you to pick any dataset of your own choosing and go through the steps mentioned above. You should try at least 3 different classes of models, and one of them should be an `xgboost` classifier or regressor. We recommend that you pick a dataset from [Kaggle](https://www.kaggle.com/). We will be very flexible in grading this lab; we just need to see that you've taken the right steps. You should submit your notebook to gradescope. Submission instructions will be given on Piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-blend",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Run the following code to install libraries and download required files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 8) # You might later get errors if you have a lower version.\n",
    "\n",
    "# Install necessary libraries\n",
    "!pip install sklearn matplotlib numpy pandas xgboost\n",
    "\n",
    "# Do all necessary imports\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary files\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def perform_downloads():\n",
    "    # Download data file\n",
    "    root_url = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "    housing_url = f\"{root_url}datasets/housing/housing.tgz\"\n",
    "    tgz_path = \"housing.tgz\"\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=\"./\")\n",
    "    housing_tgz.close()\n",
    "    \n",
    "\n",
    "def to_classification():\n",
    "    # Original dataset was for a regression tasks.\n",
    "    # Transform it to classification task.\n",
    "    classes = ['LOW', 'MEDIUM', 'HIGH']\n",
    "    housing = pd.read_csv('housing.csv')\n",
    "    housing[\"house_value_class\"] = pd.cut(housing[\"median_house_value\"], bins=[0.0, 200000, 400000, np.inf], labels=classes)\n",
    "    housing.drop('median_house_value', axis=1, inplace=True)\n",
    "    housing.to_csv(\"housing_classification.csv\", index=False)\n",
    "\n",
    "    \n",
    "    \n",
    "perform_downloads()\n",
    "to_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-rally",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "Before we begin, we should split our data into a training and a testing set. All our learning should be done on the training set. The testing set is going to be used only once after training.  \n",
    "**Q: Why is a good idea?**\n",
    "\n",
    "You could manually implement the splitting logic, but Scikit-Learn has builtin functions that do this for us. There are many splitting methodologies, but the simplest one that works well for us is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import splitting function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv(\"housing_classification.csv\")\n",
    "\n",
    "# Split data. 80% is used as training; 20% as testing. The data will be randomly split.\n",
    "train_set, test_set = train_test_split(data, test_size=0.2, random_state=37)\n",
    "\n",
    "print(f\"data size={len(data)}; training size={len(train_set)}; testing size={len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-words",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data, and take look at the attributes\n",
    "housing = train_set # Rename for convenience\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-therapy",
   "metadata": {},
   "source": [
    "Each row contains housing data aggregated by *block group*, which is how the US officially reports housing data. Our goal is to predict the housing value class (low, medium or high) given information about a block. Let's take a look at the columns within the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info gives a quick summary\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-worcester",
   "metadata": {},
   "source": [
    "First, note that the `total_bedrooms` columns contains missing values.  \n",
    "**Q: What techniques can we use to handle them?**. We'll implement one of them in the *Data Preparation* section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca12b05-6c9c-47d2-ba84-6272829991c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We also note that only the target class `house_value_class` and the feature `ocean_proximity` are categorical. In the following code, find the distinct values both columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Distinct values and count of house_value_class\n",
    "\n",
    "# TODO: Distinct values and count of ocean_proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-geneva",
   "metadata": {},
   "source": [
    "**Q: What's the standard way of dealing with categorical features in ML algorithms?**. We'll implement this in the *Data Preparation* section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b0e77-2b75-4043-8673-6dae0fc09ccd",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "As for the numerical attributes, we can quickly summarize their distribution as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-tribe",
   "metadata": {},
   "source": [
    "We see that some columns (e.g., `latitude` vs `population`) have very different ranges of values.  \n",
    "**Q: What's the standard way to deal with such differences in numerical data distributions?**. (Also to be implemented in the data prep section).\n",
    "\n",
    "----\n",
    "\n",
    "While these numbers are informative, plots are usually a lot more intuitive. Using [pandas.DataFrame.hist](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html), show a histogram for all the numerical columns.  \n",
    "**Q: Briefly describe what stands out most to you?** Hint: Look at the `housing_median_age` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot histograms (< 2min)\n",
    "# Hint: Set figure size to (15, 15), and the number of bins to 50 for good formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-increase",
   "metadata": {},
   "source": [
    "It's also possible to visualize multiple attributes together. Using [pandas.DataFrame.plot](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html), we can plot the relationship between total number of rooms and population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"population\", y=\"total_rooms\", alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8daa1-a244-479b-b199-c94a0fb44b3e",
   "metadata": {},
   "source": [
    "Naturally, we see a fairly linear, positive trend. These two values strongly depend on one another.    \n",
    "**Q: How can you reduce the number of features in the dataset using correlation?**   \n",
    "Since we don't have too many features, we won't worry about this here, but you might have to in your project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70fbc50-347d-4c23-9186-aa0206da5991",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-biology",
   "metadata": {},
   "source": [
    "More interesting visualizations are possible (there will be a lab about them). For now, take a look at the following:  \n",
    "\n",
    "**Q: What pattern does it show?** Hint: [Look at this map.](https://www.google.com/maps/place/California/@36.4998513,-121.7064629,6.55z/data=!4m5!3m4!1s0x808fb9fe5f285e3d:0x8b5109a227086f55!8m2!3d36.778261!4d-119.4179324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'LOW': 'lightblue', 'MEDIUM': 'lightyellow', 'HIGH': 'red'}\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.2, s=housing[\"population\"]/100, label=\"population\", figsize=(10,7), c=housing['house_value_class'].map(colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-smith",
   "metadata": {},
   "source": [
    "If interesting patterns exist in the dataset you choose, try showcasing them.\n",
    "\n",
    "---\n",
    "\n",
    "We are now done with data exploration. We know what the data looks like, and what preparation we need to do before moving on to training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-export",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-remedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First separate features from target variables.\n",
    "features = housing.drop(\"house_value_class\", axis=1)\n",
    "target = housing[\"house_value_class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-batman",
   "metadata": {},
   "source": [
    "We need the following transformation to make our data work with ML algorithms:\n",
    "1. One hot encoding of the categorical features. \n",
    "2. Median imputation on numerical features.\n",
    "3. Standardize numerical features.\n",
    "4. Encode class labels into ordinals (0, 1, 2 instead of LOW, MEDIUM, HIGH). This is what the ML libraries we'll use expect.\n",
    "\n",
    "`sklearn` has builtin functions for all of these. It also has convenience functions to put them all together.\n",
    "\n",
    "----\n",
    "\n",
    "One hot encoding with `sklearn` works as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Get columns to transform.\n",
    "categorical_col_names = ['ocean_proximity'] # stored as a separate variable for later.\n",
    "categorical = features[categorical_col_names]\n",
    "\n",
    "# Do one hot encoding.\n",
    "encoder = OneHotEncoder()\n",
    "categorical_one_hot = encoder.fit_transform(categorical)\n",
    "categorical_one_hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-cooking",
   "metadata": {},
   "source": [
    "As for imputation, `sklearn` has a [variety of implementations](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.impute) (corresponding to the techniques we described in class). We will stick to simple median imputation, but feel free to try something different in your custom project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Get columns to transform.\n",
    "numerical_col_names = [c for c in features.columns if c != 'ocean_proximity'] # stored as a separate variable for later.\n",
    "numerical = features[numerical_col_names]\n",
    "\n",
    "# Print missing count\n",
    "print(\"Num missing before imputation:\", np.count_nonzero(np.isnan(numerical)))\n",
    "\n",
    "# Do transformation.\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputed_numerical = imputer.fit_transform(numerical)\n",
    "\n",
    "# Print missing count. Should be 0.\n",
    "print(\"Num missing before imputation:\", np.count_nonzero(np.isnan(imputed_numerical)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-curve",
   "metadata": {},
   "source": [
    "Take a look at the documentation for [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and use it to normalize numerical columns in `imputed_numerical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TODO: Do scaling (~2 mins)\n",
    "# Hint: Reuse numerical column names from above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-arabic",
   "metadata": {},
   "source": [
    "`sklearn` has a simple API to chain all of these transformation together. Take a look at the documentation for [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and [sklearn.compose.ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Categorical Pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"one_hot_encoder\", OneHotEncoder()),\n",
    "])\n",
    "\n",
    "# TODO live: Numerical Pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "])\n",
    "\n",
    "\n",
    "# Full transformation\n",
    "feature_transformer = ColumnTransformer([\n",
    "    # Categorical\n",
    "    (\"categorical\", categorical_pipeline, categorical_col_names),\n",
    "    # TODO: Numerical\n",
    "])\n",
    "\n",
    "features_prepped = feature_transformer.fit_transform(features)\n",
    "pd.DataFrame(features_prepped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-narrative",
   "metadata": {},
   "source": [
    "Finally, we need to encode label (LOW=i, MEDIUM=j, HIGH=k) to satisfy `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "target_prepped = label_encoder.fit_transform(target)\n",
    "pd.Series(target_prepped).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-hours",
   "metadata": {},
   "source": [
    "The data is now ready for ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-university",
   "metadata": {},
   "source": [
    "## Training and Validation\n",
    "We'll now have to train and validate a few models to find the best performing one.\n",
    "We start with the simple kind of model: linear classifier. We will then give you a few minutes to try out gradient boosted trees.\n",
    "\n",
    "Training a model is simple with `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Train model\n",
    "lin_sgd = SGDClassifier(loss='hinge')\n",
    "lin_sgd.fit(features_prepped, target_prepped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-whale",
   "metadata": {},
   "source": [
    "Performing predictions is equally easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some random input\n",
    "sample = housing.sample(n=10, random_state=1)\n",
    "\n",
    "# Tranform input\n",
    "sample_prepped = feature_transformer.transform(sample)\n",
    "# Make prediction\n",
    "prediction = lin_sgd.predict(sample_prepped)\n",
    "# Decode prediction\n",
    "label_encoder.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-enterprise",
   "metadata": {},
   "source": [
    "---\n",
    "Let's validate the efficacy of the linear model. `sklearn` provides builtin k-fold cross-validation.  \n",
    "**Q: Briefly explain what k-fold cross-validation is and why it's used?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Create model\n",
    "lin_sgd = SGDClassifier(loss='hinge')\n",
    "\n",
    "# Do cross validation\n",
    "# The cv parameter is the number of cross-validation splits.\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=37)\n",
    "scores = cross_val_score(lin_sgd, features_prepped, target_prepped, cv=kfold)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (scores.mean()*100, scores.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-specific",
   "metadata": {},
   "source": [
    "We get a validation accuracy of ~$76\\%$. This may or may not be ok depending on your application. Try `xgboost.XGBClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# You'll need at least these  flags \n",
    "xgb_cls = XGBClassifier(eval_metric='mlogloss', use_label_encoder=False)\n",
    "\n",
    "# TODO: Do cross validation with xgboost. Can be somewhat slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-worth",
   "metadata": {},
   "source": [
    "You should have obtained an accuracy better than before.  \n",
    "**Q: Why do think `xgboost` performs better than linear SGD?**\n",
    "\n",
    "---\n",
    "\n",
    "**Q: Given a model, how should we tune it to improve training performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-indianapolis",
   "metadata": {},
   "source": [
    "## Testing\n",
    "We can now test our model to see how well it performs in the \"real-world\".  \n",
    "**Q: Why can't we compare our models in the testing phase?**.\n",
    "\n",
    "Fill-in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TODO: Do training on all training data.\n",
    "\n",
    "# TODO: Implementing testing code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-forge",
   "metadata": {},
   "source": [
    "Thanks to k-fold cross validation, you should see that your test error is fairly similar to your validation error. K-fold cross validation allows to predict performance on unseen data, as long its distribution is reasonably similar to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a391a-4d49-47da-9395-03437a5aa11a",
   "metadata": {},
   "source": [
    "# Go your own Way\n",
    "Now pick any dataset of your choosing, and and roughly follow the steps above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e0af3-4487-477f-8836-a18accf90735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
